

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nussl.ml.train.trainer &mdash; nussl 1.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home"> nussl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started.html">Getting Started</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials.html">Tutorials</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/examples.html">Examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../recipes/recipes.html">Recipes</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api.html">API Documentation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../contributing.html">Contribution Guide</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../changelog.html">Changelog</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">nussl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>nussl.ml.train.trainer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for nussl.ml.train.trainer</h1><div class="highlight"><pre>
<span></span>import os
import logging
from enum import Enum
import copy
import time

from ignite.engine import Events, Engine
from ignite.handlers import Timer
from torch.utils.tensorboard import SummaryWriter
import torch
from torch import nn
import numpy as np

from nussl import datasets


<div class="viewcode-block" id="ValidationEvents"><a class="viewcode-back" href="../../../../ml.html#nussl.ml.train.ValidationEvents">[docs]</a>class ValidationEvents(Enum):
    &quot;&quot;&quot;
    Events based on validation running
    &quot;&quot;&quot;
    VALIDATION_STARTED = &#39;validation_started&#39;
    VALIDATION_COMPLETED = &#39;validation_completed&#39;</div>


<div class="viewcode-block" id="BackwardsEvents"><a class="viewcode-back" href="../../../../ml.html#nussl.ml.train.BackwardsEvents">[docs]</a>class BackwardsEvents(Enum):
    &quot;&quot;&quot;
    Events based on validation running
    &quot;&quot;&quot;
    BACKWARDS_COMPLETED = &#39;backwards_completed&#39;</div>


<div class="viewcode-block" id="cache_dataset"><a class="viewcode-back" href="../../../../ml.html#nussl.ml.train.cache_dataset">[docs]</a>def cache_dataset(dataset, log_frequency=.1):
    &quot;&quot;&quot;
    Runs through an entire dataset and caches it if there nussl.datasets.transforms.Cache
    is in dataset.transform. If there is no caching, or dataset.cache_populated = True,
    then this function just iterates through the dataset and does nothing.

    This function can also take a `torch.util.data.DataLoader` object wrapped around
    a `nussl.datasets.BaseDataset` object.
    
    Args:
        dataset (nussl.datasets.BaseDataset): Must be a subclass of 
          `nussl.datasets.BaseDataset`.

        log_frequency (float, optional): How often to log progress, as a fraction between
          0.0 and 1.0 of the total dataset length. Defaults to .1 
          (10x over the course of caching).
    &quot;&quot;&quot;

    def dummy_process(engine, data):
        pass

    cache = Engine(dummy_process)
    log_frequency = max(int(len(dataset) * log_frequency), 1)

    @cache.on(Events.ITERATION_STARTED(every=log_frequency))
    def log_progress(engine):
        logging.info(
            f&quot;Cached {engine.state.iteration} / &quot;
            f&quot;{engine.state.epoch_length} batches&quot;)

    cache.run(dataset)
    dataset.cache_populated = True</div>


<div class="viewcode-block" id="create_train_and_validation_engines"><a class="viewcode-back" href="../../../../ml.html#nussl.ml.train.create_train_and_validation_engines">[docs]</a>def create_train_and_validation_engines(train_func, val_func=None, device=&#39;cpu&#39;):
    &quot;&quot;&quot;
    Helper function for creating an ignite Engine object with helpful defaults.
    This sets up an Engine that has four handlers attached to it:

    - prepare_batch: before a batch is passed to train_func or val_func, this
      function runs, moving every item in the batch (which is a dictionary) to
      the appropriate device (&#39;cpu&#39;  or &#39;cuda&#39;).

    - book_keeping: sets up some dictionaries that are used for bookkeeping so one
      can easily track the epoch and iteration losses for both training and
      validation.

    - add_to_iter_history: records the iteration, epoch, and past iteration losses
      into the dictionaries set up by book_keeping.

    - clear_iter_history: resets the current iteration history of losses after moving
      the current iteration history into past iteration history.
    
    Args:
        train_func (func): Function that provides the closure for training for
          a single batch.
        val_func (func, optional): Function that provides the closure for
          validating a single batch. Defaults to None.
        device (str, optional): Device to move tensors to. Defaults to &#39;cpu&#39;.
    &quot;&quot;&quot;
    # Set up engines for training and validation
    trainer = Engine(train_func)
    trainer.register_events(*ValidationEvents)
    trainer.register_events(*BackwardsEvents)

    validator = None if val_func is None else Engine(val_func)

    # Before a batch starts, the items should be float and moved to the 
    # correct device, for both training and validation. Checks to make
    # sure &quot;cuda&quot; is available if user requested cuda.
    device = torch.device(device)
    if not torch.cuda.is_available():
        device = torch.device(&#39;cpu&#39;)

    def prepare_batch(engine):
        batch = engine.state.batch
        for key in batch:
            if torch.is_tensor(batch[key]):
                batch[key] = batch[key].float().to(device)
        engine.state.batch = batch

    # Set up stuff for bookkeeping as training progresses.
    def book_keeping(engine):
        engine.state.epoch_history = {}
        engine.state.iter_history = {}
        engine.state.past_iter_history = {}

    def add_to_iter_history(engine):
        for key in engine.state.output:
            if key not in engine.state.iter_history:
                engine.state.iter_history[key] = []
            if key not in engine.state.past_iter_history:
                engine.state.past_iter_history[key] = []
            engine.state.iter_history[key].append(
                engine.state.output[key]
            )
            engine.state.past_iter_history[key].append(
                engine.state.iter_history[key]
            )

    def clear_iter_history(engine):
        engine.state.iter_history = {}

    trainer.add_event_handler(
        Events.ITERATION_STARTED, prepare_batch)
    trainer.add_event_handler(
        Events.STARTED, book_keeping)
    trainer.add_event_handler(
        Events.ITERATION_COMPLETED, add_to_iter_history)
    trainer.add_event_handler(
        Events.EPOCH_STARTED, clear_iter_history)

    if validator is not None:
        validator.add_event_handler(
            Events.ITERATION_STARTED, prepare_batch)
        validator.add_event_handler(
            Events.STARTED, book_keeping)
        validator.add_event_handler(
            Events.ITERATION_COMPLETED, add_to_iter_history)
        validator.add_event_handler(
            Events.EPOCH_STARTED, clear_iter_history)

    return trainer, validator</div>


<div class="viewcode-block" id="add_validate_and_checkpoint"><a class="viewcode-back" href="../../../../ml.html#nussl.ml.train.add_validate_and_checkpoint">[docs]</a>def add_validate_and_checkpoint(output_folder, model, optimizer, train_data, trainer,
                                val_data=None, validator=None):
    &quot;&quot;&quot;
    This adds the following handler to the trainer:

    - validate_and_checkpoint: this runs the validator on the validation dataset 
      (``val_data``) using a defined validation process function ``val_func``. 
      These are optional. If these are not provided, then no validator is run
      and the model is simply checkpointed. The model is always saved to 
      ``{output_folder}/checkpoints/latest.model.pth``. If the model is also the 
      one with the lowest validation loss, then it is *also* saved to
      ``{output_folder}/checkpoints/best.model.pth. This is attached to
      ``Events.EPOCH_COMPLETED`` on the trainer. After completion, it fires a
      ``ValidationEvents.VALIDATION_COMPLETED`` event.

    Args:
        model (torch.nn.Module): Model that is being trained (typically a SeparationModel).
          optimizer (torch.optim.Optimizer): Optimizer being used to train.

        train_data (BaseDataset): dataset that is being used to train the model. This is to
          save additional metadata information alongside the model checkpoint such as the
          STFTParams, dataset folder, length, list of transforms, etc.

        trainer (ignite.Engine): Engine for trainer

        validator (ignite.Engine, optional): Engine for validation. 
          Defaults to None.

        val_data (torch.utils.data.Dataset, optional): The validation data. 
          Defaults to None.
    &quot;&quot;&quot;

    # When the trainer finishes an epoch, it should validate and save 
    # the model.
    @trainer.on(Events.EPOCH_COMPLETED)
    def validate_and_checkpoint(trainer):
        trainer.fire_event(ValidationEvents.VALIDATION_STARTED)

        is_best = True
        if validator is not None:
            validator.run(val_data)

            for key in validator.state.iter_history:
                _key = f&quot;validation/{key}&quot;
                if _key not in trainer.state.epoch_history:
                    trainer.state.epoch_history[_key] = []
                trainer.state.epoch_history[_key].append(np.mean(
                    validator.state.iter_history[key]
                ))

            if &#39;validation/loss&#39; in trainer.state.epoch_history:
                cur = trainer.state.epoch_history[&#39;validation/loss&#39;][-1]
                is_best = cur == min(trainer.state.epoch_history[&#39;validation/loss&#39;])

        for key in trainer.state.iter_history:
            _key = f&quot;train/{key}&quot;
            if _key not in trainer.state.epoch_history:
                trainer.state.epoch_history[_key] = []
            trainer.state.epoch_history[_key].append(np.mean(
                trainer.state.iter_history[key]
            ))

        output_paths = [os.path.join(
            output_folder, &#39;checkpoints&#39;, &#39;latest.model.pth&#39;)]
        if is_best:
            output_paths.append(os.path.join(
                output_folder, &#39;checkpoints&#39;, &#39;best.model.pth&#39;
            ))

        _transform = copy.deepcopy(train_data.transform)

        if isinstance(_transform, datasets.transforms.Compose):
            for t in _transform.transforms:
                if isinstance(t, datasets.transforms.Cache):
                    _transform.transforms.remove(t)

        metadata = {
            &#39;stft_params&#39;: train_data.stft_params,
            &#39;sample_rate&#39;: train_data.sample_rate,
            &#39;num_channels&#39;: train_data.num_channels,
            &#39;folder&#39;: train_data.folder,
            &#39;transforms&#39;: _transform,
            &#39;trainer.state_dict&#39;: {
                &#39;epoch&#39;: trainer.state.epoch,
                &#39;epoch_length&#39;: trainer.state.epoch_length,
                &#39;max_epochs&#39;: trainer.state.max_epochs,
                &#39;output&#39;: trainer.state.output,
                &#39;metrics&#39;: trainer.state.metrics,
                &#39;seed&#39;: trainer.state.seed,
            },
            &#39;trainer.state.epoch_history&#39;: trainer.state.epoch_history,
        }

        for _path in output_paths:
            os.makedirs(os.path.join(
                output_folder, &#39;checkpoints&#39;), exist_ok=True)
            if isinstance(model, nn.DataParallel):
                _model = model.module
            else:
                _model = model
            _model.save(_path, {&#39;metadata&#39;: metadata})
            torch.save(optimizer.state_dict(),
                       _path.replace(&#39;model.pth&#39;, &#39;optimizer.pth&#39;))

        trainer.state.saved_model_path = output_paths[-1]
        trainer.state.output_folder = output_folder
        trainer.fire_event(ValidationEvents.VALIDATION_COMPLETED)</div>


<div class="viewcode-block" id="add_stdout_handler"><a class="viewcode-back" href="../../../../ml.html#nussl.ml.train.add_stdout_handler">[docs]</a>def add_stdout_handler(trainer, validator=None):
    &quot;&quot;&quot;
    This adds the following handler to the trainer engine, and also sets up
    Timers:

    - log_epoch_to_stdout: This logs the results of a model after it has trained
      for a single epoch on both the training and validation set. The output typically
      looks like this:

      .. code-block:: none

            EPOCH SUMMARY
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            - Epoch number: 0010 / 0010
            - Training loss:   0.583591
            - Validation loss: 0.137209
            - Epoch took: 00:00:03
            - Time since start: 00:00:32
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            Saving to test.
            Output @ tests/local/trainer
    
    Args:
        trainer (ignite.Engine): Engine for trainer

        validator (ignite.Engine, optional): Engine for validation. 
          Defaults to None.
    &quot;&quot;&quot;
    # Set up timers for overall time taken and each epoch
    overall_timer = Timer(average=False)
    overall_timer.attach(trainer,
                         start=Events.STARTED, pause=Events.COMPLETED)

    epoch_timer = Timer(average=False)
    epoch_timer.attach(
        trainer, start=Events.EPOCH_STARTED,
        pause=ValidationEvents.VALIDATION_COMPLETED
    )

    @trainer.on(ValidationEvents.VALIDATION_COMPLETED)
    def log_epoch_to_stdout(trainer):
        epoch_time = epoch_timer.value()
        epoch_time = time.strftime(
            &quot;%H:%M:%S&quot;, time.gmtime(epoch_time))
        overall_time = overall_timer.value()
        overall_time = time.strftime(
            &quot;%H:%M:%S&quot;, time.gmtime(overall_time))

        epoch_number = trainer.state.epoch
        total_epochs = trainer.state.max_epochs

        try:
            validation_loss = (
                f&quot;{trainer.state.epoch_history[&#39;validation/loss&#39;][-1]:04f}&quot;)
        except:
            validation_loss = &#39;N/A&#39;

        train_loss = trainer.state.epoch_history[&#39;train/loss&#39;][-1]
        saved_model_path = trainer.state.saved_model_path

        logging_str = (
            f&quot;\n\n&quot;
            f&quot;EPOCH SUMMARY \n&quot;
            f&quot;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n&quot;
            f&quot;- Epoch number: {epoch_number:04d} / {total_epochs:04d} \n&quot;
            f&quot;- Training loss:   {train_loss:04f} \n&quot;
            f&quot;- Validation loss: {validation_loss} \n&quot;
            f&quot;- Epoch took: {epoch_time} \n&quot;
            f&quot;- Time since start: {overall_time} \n&quot;
            f&quot;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n&quot;
            f&quot;Saving to {saved_model_path}. \n&quot;
            f&quot;Output @ {trainer.state.output_folder} \n&quot;
        )

        logging.info(logging_str)</div>


<div class="viewcode-block" id="add_tensorboard_handler"><a class="viewcode-back" href="../../../../ml.html#nussl.ml.train.add_tensorboard_handler">[docs]</a>def add_tensorboard_handler(output_folder, engine):
    &quot;&quot;&quot;
    Every key in engine.state.epoch_history[-1] is logged to TensorBoard.
    
    Args:
        output_folder (str): Where the tensorboard logs should go. The output
          logs will go into a subfolder &#39;tensorboard&#39;.

        trainer (ignite.Engine): The engine to log.
    &quot;&quot;&quot;

    @engine.on(ValidationEvents.VALIDATION_COMPLETED)
    def log_to_tensorboard(engine):
        writer = SummaryWriter(
            os.path.join(output_folder, &#39;tensorboard&#39;))
        for key in engine.state.epoch_history:
            writer.add_scalar(
                key, engine.state.epoch_history[key][-1], engine.state.epoch)</div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ethan Manilow, Prem Seetharaman

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>