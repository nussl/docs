

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nussl.ml.confidence &mdash; nussl 1.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> nussl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started.html">Getting Started</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials.html">Tutorials</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/examples.html">Examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../recipes/recipes.html">Recipes</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API Documentation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../citation.html">Citing nussl</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contribution Guide</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../changelog.html">Changelog</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">nussl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>nussl.ml.confidence</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for nussl.ml.confidence</h1><div class="highlight"><pre>
<span></span>&quot;&quot;&quot;
There are ways to measure the quality of a separated source without
requiring ground truth. These functions operate on the output of
clustering-based separation algorithms and work by analyzing
the clusterability of the feature space used to generate the
separated sources.
&quot;&quot;&quot;

from sklearn.metrics import silhouette_samples
import numpy as np
from .cluster import KMeans, GaussianMixture
from scipy.special import logsumexp
from .train import loss
import torch

def softmax(x, axis=None):
    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))

<div class="viewcode-block" id="jensen_shannon_divergence"><a class="viewcode-back" href="../../../ml.html#nussl.ml.confidence.jensen_shannon_divergence">[docs]</a>def jensen_shannon_divergence(gmm_p, gmm_q, n_samples=10**5):
    &quot;&quot;&quot;
    Compute Jensen-Shannon (JS) divergence between two Gaussian Mixture Models via 
    sampling. JS divergence is also known as symmetric Kullback-Leibler divergence.
    JS divergence has no closed form in general for GMMs, thus we use sampling to 
    compute it.

    Args:
        gmm_p (GaussianMixture): A GaussianMixture class fit to some data.
        gmm_q (GaussianMixture): Another GaussianMixture class fit to some data.
        n_samples (int): Number of samples to use to estimate JS divergence. 

    Returns:
        JS divergence between gmm_p and gmm_q
    &quot;&quot;&quot;
    X = gmm_p.sample(n_samples)[0]
    log_p_X = gmm_p.score_samples(X)
    log_q_X = gmm_q.score_samples(X)
    log_mix_X = np.logaddexp(log_p_X, log_q_X)

    Y = gmm_q.sample(n_samples)[0]
    log_p_Y = gmm_p.score_samples(Y)
    log_q_Y = gmm_q.score_samples(Y)
    log_mix_Y = np.logaddexp(log_p_Y, log_q_Y)

    return (log_p_X.mean() - (log_mix_X.mean() - np.log(2))
            + log_q_Y.mean() - (log_mix_Y.mean() - np.log(2))) / 2</div>

def _get_loud_bins_mask(threshold, audio_signal=None, representation=None):
    if representation is None:
        representation = np.abs(audio_signal.stft())
    threshold = np.percentile(representation, threshold)
    mask = representation &gt; threshold
    return mask, representation

<div class="viewcode-block" id="jensen_shannon_confidence"><a class="viewcode-back" href="../../../ml.html#nussl.ml.confidence.jensen_shannon_confidence">[docs]</a>def jensen_shannon_confidence(audio_signal, features, num_sources, threshold=95, 
                              n_samples=10**5, **kwargs):
    &quot;&quot;&quot;
    Calculates the clusterability of a space by comparing a K-cluster GMM
    with a 1-cluster GMM on the same features. This function fits two
    GMMs to all of the points that are above the specified threshold (defaults
    to 95: 95th percentile of all the data). This saves on computation time and
    also allows one to have the confidence measure only focus on the louder
    more perceptually important points.

    References:

    Seetharaman, Prem, Gordon Wichern, Jonathan Le Roux, and Bryan Pardo. 
    “Bootstrapping Single-Channel Source Separation via Unsupervised Spatial 
    Clustering on Stereo Mixtures”. 44th International Conference on Acoustics, 
    Speech, and Signal Processing, Brighton, UK, May, 2019

    Seetharaman, Prem. Bootstrapping the Learning Process for Computer Audition. 
    Diss. Northwestern University, 2019.
    
    Args:
        audio_signal (AudioSignal): AudioSignal object which will be used to compute
          the mask over which to compute the confidence measure. This can be None, if
          and only if ``representation`` is passed as a keyword argument to this 
          function.
        features (np.ndarray): Numpy array containing the features to be clustered. 
          Should have the same dimensions as the representation.
        n_sources (int): Number of sources to cluster the features into.
        threshold (int, optional): Threshold by loudness. Points below the threshold are
          excluded from being used in the confidence measure. Defaults to 95.
        kwargs: Keyword arguments to `_get_loud_bins_mask`. Namely, representation can
          go here as a keyword argument.

    Returns:
        float: Confidence given by Jensen-Shannon divergence.
    &quot;&quot;&quot;
    mask, _ = _get_loud_bins_mask(threshold, audio_signal, **kwargs)
    embedding_size = features.shape[-1]
    features = features[mask].reshape(-1, embedding_size)

    one_component_gmm = GaussianMixture(1)
    n_component_gmm = GaussianMixture(num_sources)

    one_component_gmm.fit(features)
    n_component_gmm.fit(features)

    confidence = jensen_shannon_divergence(
        one_component_gmm, n_component_gmm, n_samples=n_samples)

    return confidence</div>

<div class="viewcode-block" id="posterior_confidence"><a class="viewcode-back" href="../../../ml.html#nussl.ml.confidence.posterior_confidence">[docs]</a>def posterior_confidence(audio_signal, features, num_sources, threshold=95, 
                         **kwargs):
    &quot;&quot;&quot;
    Calculates the clusterability of an embedding space by looking at the
    strength of the assignments of each point to a specific cluster. The 
    more points that are &quot;in between&quot; clusters (e.g. no strong assignmment),
    the lower the clusterability.

    References:

    Seetharaman, Prem, Gordon Wichern, Jonathan Le Roux, and Bryan Pardo. 
    “Bootstrapping Single-Channel Source Separation via Unsupervised Spatial 
    Clustering on Stereo Mixtures”. 44th International Conference on Acoustics, 
    Speech, and Signal Processing, Brighton, UK, May, 2019

    Seetharaman, Prem. Bootstrapping the Learning Process for Computer Audition. 
    Diss. Northwestern University, 2019.
    
    Args:
        audio_signal (AudioSignal): AudioSignal object which will be used to compute
          the mask over which to compute the confidence measure. This can be None, if
          and only if ``representation`` is passed as a keyword argument to this 
          function.
        features (np.ndarray): Numpy array containing the features to be clustered. 
          Should have the same dimensions as the representation.
        n_sources (int): Number of sources to cluster the features into.
        threshold (int, optional): Threshold by loudness. Points below the threshold are
          excluded from being used in the confidence measure. Defaults to 95.
        kwargs: Keyword arguments to `_get_loud_bins_mask`. Namely, representation can
          go here as a keyword argument.
    
    Returns:
        float: Confidence given by posteriors.
    &quot;&quot;&quot;
    mask, _ = _get_loud_bins_mask(threshold, audio_signal, **kwargs)
    embedding_size = features.shape[-1]
    features = features[mask].reshape(-1, embedding_size)

    kmeans = KMeans(num_sources)
    distances = kmeans.fit_transform(features)

    confidence = softmax(-distances, axis=-1)

    confidence = (
        (num_sources * np.max(confidence, axis=-1) - 1) / 
        (num_sources - 1)
    )

    return confidence.mean()</div>

<div class="viewcode-block" id="silhouette_confidence"><a class="viewcode-back" href="../../../ml.html#nussl.ml.confidence.silhouette_confidence">[docs]</a>def silhouette_confidence(audio_signal, features, num_sources, threshold=95, 
                          max_points=1000, **kwargs):
    &quot;&quot;&quot;
    Uses the silhouette score to compute the clusterability of the feature space.

    The Silhouette Coefficient is calculated using the 
    mean intra-cluster distance (a) and the mean nearest-cluster distance (b) 
    for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). 
    To clarify, b is the distance between a sample and the nearest cluster 
    that the sample is not a part of. Note that Silhouette Coefficient is 
    only defined if number of labels is 2 &lt;= n_labels &lt;= n_samples - 1.

    References:

    Seetharaman, Prem. Bootstrapping the Learning Process for Computer Audition. 
    Diss. Northwestern University, 2019.

    Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the 
    Interpretation and Validation of Cluster Analysis”. Computational and 
    Applied Mathematics 20: 53-65.
    
    Args:
        audio_signal (AudioSignal): AudioSignal object which will be used to compute
          the mask over which to compute the confidence measure. This can be None, if
          and only if ``representation`` is passed as a keyword argument to this 
          function.
        features (np.ndarray): Numpy array containing the features to be clustered. 
          Should have the same dimensions as the representation.
        n_sources (int): Number of sources to cluster the features into.
        threshold (int, optional): Threshold by loudness. Points below the threshold are
          excluded from being used in the confidence measure. Defaults to 95.
        kwargs: Keyword arguments to `_get_loud_bins_mask`. Namely, representation can
          go here as a keyword argument.
        max_points (int, optional): Maximum number of points to compute the Silhouette
          score for. Silhouette score is a costly operation. Defaults to 1000.
    
    Returns:
        float: Confidence given by Silhouette score.
    &quot;&quot;&quot;
    mask, _ = _get_loud_bins_mask(threshold, audio_signal, **kwargs)
    embedding_size = features.shape[-1]
    features = features[mask].reshape(-1, embedding_size)

    if features.shape[0] &gt; max_points:
        idx = np.random.choice(
            np.arange(features.shape[0]), max_points,
            replace=False)
        features = features[idx]
    
    kmeans = KMeans(num_sources)

    labels = kmeans.fit_predict(features)
    confidence = silhouette_samples(features, labels)

    return confidence.mean()</div>

<div class="viewcode-block" id="loudness_confidence"><a class="viewcode-back" href="../../../ml.html#nussl.ml.confidence.loudness_confidence">[docs]</a>def loudness_confidence(audio_signal, features, num_sources, threshold=95, 
                        **kwargs):
    &quot;&quot;&quot;
    Computes the clusterability of the feature space by comparing the absolute
    size of each cluster.
    
    References:

    Seetharaman, Prem, Gordon Wichern, Jonathan Le Roux, and Bryan Pardo. 
    “Bootstrapping Single-Channel Source Separation via Unsupervised Spatial 
    Clustering on Stereo Mixtures”. 44th International Conference on Acoustics, 
    Speech, and Signal Processing, Brighton, UK, May, 2019

    Seetharaman, Prem. Bootstrapping the Learning Process for Computer Audition. 
    Diss. Northwestern University, 2019.
    
    Args:
        audio_signal (AudioSignal): AudioSignal object which will be used to compute
          the mask over which to compute the confidence measure. This can be None, if
          and only if ``representation`` is passed as a keyword argument to this 
          function.
        features (np.ndarray): Numpy array containing the features to be clustered. 
          Should have the same dimensions as the representation.
        n_sources (int): Number of sources to cluster the features into.
        threshold (int, optional): Threshold by loudness. Points below the threshold are
          excluded from being used in the confidence measure. Defaults to 95.
        kwargs: Keyword arguments to `_get_loud_bins_mask`. Namely, representation can
          go here as a keyword argument.
    
    Returns:
        float: Confidence given by size of smallest cluster.
    &quot;&quot;&quot;
    mask, _ = _get_loud_bins_mask(threshold, audio_signal, **kwargs)
    embedding_size = features.shape[-1]
    features = features[mask].reshape(-1, embedding_size)

    kmeans = KMeans(num_sources)
    labels = kmeans.fit_predict(features)

    source_shares = np.array(
        [(labels == i).sum() for i in range(num_sources)]
    ).astype(float)
    source_shares *= (1 / source_shares.sum())
    confidence = source_shares.min()

    return confidence</div>

<div class="viewcode-block" id="whitened_kmeans_confidence"><a class="viewcode-back" href="../../../ml.html#nussl.ml.confidence.whitened_kmeans_confidence">[docs]</a>def whitened_kmeans_confidence(audio_signal, features, num_sources, threshold=95, 
                               **kwargs):
    &quot;&quot;&quot;
    Computes the clusterability in two steps:

    1. Cluster the feature space using KMeans into assignments
    2. Compute the Whitened K-Means loss between the features and the assignments.
    
    Args:
        audio_signal (AudioSignal): AudioSignal object which will be used to compute
          the mask over which to compute the confidence measure. This can be None, if
          and only if ``representation`` is passed as a keyword argument to this 
          function.
        features (np.ndarray): Numpy array containing the features to be clustered. 
          Should have the same dimensions as the representation.
        n_sources (int): Number of sources to cluster the features into.
        threshold (int, optional): Threshold by loudness. Points below the threshold are
          excluded from being used in the confidence measure. Defaults to 95.
        kwargs: Keyword arguments to `_get_loud_bins_mask`. Namely, representation can
          go here as a keyword argument.
    
    Returns:
        float: Confidence given by whitened k-means loss.
    &quot;&quot;&quot;
    mask, representation = _get_loud_bins_mask(threshold, audio_signal, **kwargs)
    embedding_size = features.shape[-1]
    features = features[mask].reshape(-1, embedding_size)
    weights = representation[mask].reshape(-1)

    kmeans = KMeans(num_sources)
    distances = kmeans.fit_transform(features)
    assignments = (distances == distances.max(axis=-1, keepdims=True))

    loss_func = loss.WhitenedKMeansLoss()

    features = torch.from_numpy(features).unsqueeze(0).float()
    assignments = torch.from_numpy(assignments).unsqueeze(0).float()
    weights = torch.from_numpy(weights).unsqueeze(0).float()

    loss_val = loss_func(features, assignments, weights).item()
    upper_bound = embedding_size + num_sources
    confidence = 1 - (loss_val / upper_bound)
    return confidence</div>

<div class="viewcode-block" id="dpcl_classic_confidence"><a class="viewcode-back" href="../../../ml.html#nussl.ml.confidence.dpcl_classic_confidence">[docs]</a>def dpcl_classic_confidence(audio_signal, features, num_sources, threshold=95, 
                            **kwargs):
    &quot;&quot;&quot;
    Computes the clusterability in two steps:

    1. Cluster the feature space using KMeans into assignments
    2. Compute the classic deep clustering loss between the features and the assignments.
    
    Args:
        audio_signal (AudioSignal): AudioSignal object which will be used to compute
          the mask over which to compute the confidence measure. This can be None, if
          and only if ``representation`` is passed as a keyword argument to this 
          function.
        features (np.ndarray): Numpy array containing the features to be clustered. 
          Should have the same dimensions as the representation.
        n_sources (int): Number of sources to cluster the features into.
        threshold (int, optional): Threshold by loudness. Points below the threshold are
          excluded from being used in the confidence measure. Defaults to 95.
        kwargs: Keyword arguments to `_get_loud_bins_mask`. Namely, representation can
          go here as a keyword argument.
    
    Returns:
        float: Confidence given by deep clustering loss.
    &quot;&quot;&quot;
    mask, representation = _get_loud_bins_mask(threshold, audio_signal, **kwargs)
    embedding_size = features.shape[-1]
    features = features[mask].reshape(-1, embedding_size)
    weights = representation[mask].reshape(-1)

    kmeans = KMeans(num_sources)
    distances = kmeans.fit_transform(features)
    assignments = (distances == distances.max(axis=-1, keepdims=True))

    loss_func = loss.DeepClusteringLoss()

    features = torch.from_numpy(features).unsqueeze(0).float()
    assignments = torch.from_numpy(assignments).unsqueeze(0).float()
    weights = torch.from_numpy(weights).unsqueeze(0).float()

    loss_val = loss_func(features, assignments, weights).item()
    confidence = 1 - loss_val
    return confidence</div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ethan Manilow, Prem Seetharaman

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>