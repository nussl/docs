

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nussl.evaluation.evaluation_base &mdash; nussl 1.0.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> nussl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started.html">Getting Started</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials.html">Tutorials</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/examples.html">Examples</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../recipes/recipes.html">Recipes</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API Documentation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contribution Guide</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../changelog.html">Changelog</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">nussl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>nussl.evaluation.evaluation_base</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for nussl.evaluation.evaluation_base</h1><div class="highlight"><pre>
<span></span>import os
import json
from itertools import permutations, combinations

import pandas as pd
import numpy as np

from .. import AudioSignal
from ..core import utils


def aggregate_score_files(json_files, aggregator=np.median):
    &quot;&quot;&quot;
    Takes a list of json files output by an Evaluation method in nussl
    and aggregates all the metrics into a Pandas dataframe. Sample
    output:

    .. code-block:: none

                                SDR        SIR        SAR
        drums  oracle0.json   9.086025  15.025801  10.362709
               random0.json  -6.539877  -6.087538   3.508338
               oracle1.json   9.591432  14.335700  11.365882
               random1.json  -1.358840  -0.993666   9.577297
        bass   oracle0.json   7.936720  12.843092   9.631929
               random0.json  -4.190299  -3.730649   5.802003
               oracle1.json   8.581090  12.513445  10.831370
               random1.json   0.365171   0.697621  11.693103
        other  oracle0.json   2.024207   6.133359   4.158805
               random0.json  -9.857085  -9.481909   0.965199
               oracle1.json   3.961383   6.861785   7.085745
               random1.json  -4.042277  -3.707997   7.260934
        vocals oracle0.json  12.169686  16.650161  14.085037
               random0.json  -2.440166  -1.884026   6.760966
               oracle1.json  12.409913  16.248470  14.725983
               random1.json   1.609577   1.958037  12.738970
    
    Args:
        json_files (list): List of JSON files that will be parsed for metrics.
        aggregator ([type], optional): How to aggregate results within a single
          track. Defaults to np.median.
    
    Returns:
        pd.DataFrame: Pandas dataframe containing the aggregated metrics.
    &quot;&quot;&quot;
    metrics = {}
    for json_file in json_files:
        with open(json_file, &#39;r&#39;) as f:
            data = json.load(f)
        json_key = os.path.basename(json_file)
        for name in data:
            if name not in [&#39;combination&#39;, &#39;permutation&#39;]:
                if name not in metrics:
                    metrics[name] = {}
                if json_key not in metrics[name]:
                    metrics[name][json_key] = {}
                for key in data[name]:
                    _data = aggregator(data[name][key])
                    metrics[name][json_key][key] = _data
    
    df = pd.concat({
        k: pd.DataFrame(v).T for k, v in metrics.items()
    }, axis=0, names=[&#39;source&#39;, &#39;file&#39;])
    return df


<div class="viewcode-block" id="EvaluationBase"><a class="viewcode-back" href="../../../evaluation.html#nussl.evaluation.EvaluationBase">[docs]</a>class EvaluationBase(object):
    &quot;&quot;&quot;
    Base class for all Evaluation classes for source separation algorithms in nussl. 
    Contains common functions for all evaluation techniques. This class should not be 
    instantiated directly.
    
    Both ``true_sources_list`` and ``estimated_sources_list`` get validated 
    using the private method :func:`_verify_input_list`. If your evaluation 
    needs to verify that input is set correctly (recommended) overwrite that method 
    to add checking.
    
    Args:
        true_sources_list (list): List of objects that contain one ground truth source per object.
            In some instances (such as the :class:`BSSEval` objects) this list is filled with
            :class:`AudioSignals` but in other cases it is populated with
            :class:`MaskBase` -derived objects (i.e., either a :class:`BinaryMask` or
            :class:`SoftMask` object).
        estimated_sources_list (list): List of objects that contain source estimations from a source
            separation algorithm. List should be populated with the same type of objects and in the
            same order as :param:`true_sources_list`.
        source_labels (list): List of strings that are labels for each source to be used as keys for
            the scores. Default value is `None` and in that case labels use the file_name attribute.
            If that is also `None`, then the source labels are `Source 0`, `Source 1`, etc.
        compute_permutation (bool): Whether or not to evaluate in a permutation-invariant 
            fashion, where the estimates are permuted to match the true sources. Only the 
            best permutation according to ``best_permutation_key`` is returned to the 
            scores dict. Defaults to False.
        best_permutation_key (str): Which metric to use to decide which permutation of 
            the sources was best.
        **kwargs (dict): Any additional keyword arguments are passed on to ``evaluate_helper``.
    &quot;&quot;&quot;

    def __init__(self, true_sources_list, estimated_sources_list, source_labels=None, 
                 compute_permutation=False, best_permutation_key=None, **kwargs):
        self.true_sources_list = self._verify_input_list(true_sources_list)
        self.estimated_sources_list = self._verify_input_list(estimated_sources_list)

        _source_labels = []
        for i, x in enumerate(self.true_sources_list):
            if isinstance(x, AudioSignal) and x.path_to_input_file:
                label = x.path_to_input_file
            else:
                label = f&#39;source_{i}&#39;
            _source_labels.append(label)

        if source_labels:
            for i, label in enumerate(source_labels):
                _source_labels[i] = label

        self.source_labels = _source_labels
        self.eval_args = kwargs
        self.evaluation_object_type = type(self.true_sources_list[0])
        self._scores = {}
        self.num_channels = self.true_sources_list[0].num_channels
        self.compute_permutation = compute_permutation
        self.best_permutation_key = best_permutation_key

    @staticmethod
    def _verify_input_list(audio_signal_list):
        &quot;&quot;&quot;
        Base method for verifying a list of input objects for an :class:`EvaluationBase`-derived
        object. Override this method when creating new :class:`EvaluationBased`-derived class.
        
        By default calls :func:`nussl.utils.verify_audio_signal_list_strict`, which verifies that
        all objects in the input list are :class:`audio_signal.AudioSignal` objects with the same
        length, sample rate and have identical number of channels.
        
        Args:
            audio_signal_list (list): List of objects that contain one signal per object. In some
                instances (such as the :class:`BSSEval`) this list is filled with
                :ref:`AudioSignals` but in other cases it is populated with :class:`MaskBase`
                -derived objects (i.e., either a :class:`BinaryMask` or :class:`SoftMask`
                object). In the latter case, this method is overridden with a specific function 
                in :class:`evaluation.precision_recall_fscore.PrecisionRecallFScore`.

        Returns:
            A verified list of objects that are ready for running the evaluation method.

        &quot;&quot;&quot;
        return utils.verify_audio_signal_list_strict(audio_signal_list)

<div class="viewcode-block" id="EvaluationBase.preprocess"><a class="viewcode-back" href="../../../evaluation.html#nussl.evaluation.EvaluationBase.preprocess">[docs]</a>    def preprocess(self):
        &quot;&quot;&quot;
        Takes the objects contained in `true_sources_list` and `estimated_sources_list`
        and processes them into numpy arrays that have shape 
        (..., n_channels, n_sources).

        Returns:
            references, estimates in that order as np.ndarrays.

        Note:
            Make sure to return the preprocessed data in the order 
            (references, estimates)!
        &quot;&quot;&quot;
        raise NotImplementedError(&#39;Must implement preprocess in subclass!&#39;)</div>
    
<div class="viewcode-block" id="EvaluationBase.get_candidates"><a class="viewcode-back" href="../../../evaluation.html#nussl.evaluation.EvaluationBase.get_candidates">[docs]</a>    def get_candidates(self):
        &quot;&quot;&quot;
        This gets all the possible candidates for evaluation. If `compute_permutation`
        is False, then the estimates and the references are assumed to be in the same
        order. The first N estimates will be compared to the first N references, where
        N is min(len(estimates), len(references)).

        If `compute_permutation` is True, and `len(estimates) == len(references)`, then
        every possible ordering of the estimates will be tried to match to the references.
        So if there are 3 references and 3 estimates, a total of 3! = 6 candidates will
        be generated.

        If `compute_permutation` is True and `len(estimates) &gt; len(references)`, then
        every combination of size `len(references)` estimates will be tried as well
        as their permutations. If there are 2 references and 4 estimates, then 
        (4 choose 2) = 6 combos will be tried. For each of those pairs of 2, there will
        be 2! = 2 permutations. So a total of 12 candidates will be generated.

        Returns:
            Two lists of combinations and permutations that should be tried. Each element
            of the list contains the indices that are used to find the sources that
            are compared to each other.

        &quot;&quot;&quot;
        num_sources = len(self.true_sources_list)
        num_estimates = len(self.estimated_sources_list)

        if not self.compute_permutation:
            return [list(range(num_estimates))], [list(range(num_estimates))]

        combos = list(combinations(range(num_estimates), num_sources))
        orderings = list(permutations(range(num_sources)))
        return combos, orderings</div>

<div class="viewcode-block" id="EvaluationBase.evaluate_helper"><a class="viewcode-back" href="../../../evaluation.html#nussl.evaluation.EvaluationBase.evaluate_helper">[docs]</a>    def evaluate_helper(self, references, estimates, **kwargs):
        &quot;&quot;&quot;
        This function should be implemented by each class that inherits this
        class. The function should take in a numpy array containing the references and 
        one for the estimates and compute evaluation measures between the two arrays. 
        The results should be stored in a list of dictionaries. For example, a 
        BSSEval evaluator may return a dictionary as follows, for a single estimate:

        .. code-block:: none

                           #or windows or both
            [   {          #ch0  ch1   # results for first estimate
                    &quot;SDR&quot;: [5.6, 5.2], # metric
                    &quot;SIR&quot;: [9.2, 8.9], # metric
                    &quot;SAR&quot;: [4.1, 4.3]  # metric
                }, 
                ...                    # more results for other estimates
            ]

        Each metric should be a key in the dictionary pointing to a value which is a
        list. The list will contain the metrics for however the algorithm was implemented 
        (e.g. there might be two value, one for each channel in a stereo mix, or there
        might be a sequence, one for each window that was evaluated.)
        
        Args:
            references (np.ndarray): References kept in a numpy array. Should have shape
                (..., n_channels, n_sources).
            estimates (np.ndarray): Estimates kept in whatever format you want. Should have
                shape (..., n_channels, n_sources).
            kwargs (dict): Keyword arguments with any additional arguments to be used in 
                the function (e.g. window_size, hop_length).
        
        Returns:
            A list of dictionary containing the measures corresponding to each estimate
            and reference.
        &quot;&quot;&quot;

        raise NotImplementedError(&#39;Must implement evaluate_helper in a subclass!&#39;)</div>

<div class="viewcode-block" id="EvaluationBase.evaluate"><a class="viewcode-back" href="../../../evaluation.html#nussl.evaluation.EvaluationBase.evaluate">[docs]</a>    def evaluate(self):
        &quot;&quot;&quot;
        This function encapsulates the main functionality of all evaluation classes.
        It performs the following steps, some of which must be implemented in subclasses
        of EvaluationBase.

            1. Preprocesses the data somehow into numpy arrays that get passed into your
               evaluation function.
            2. Gets all possible candidates that will be evaluated in your evaluation function.
            3. For each candidate, runs the evaluation function (must be implemented in subclass).
            4. Finds the results from the best candidate.
            5. Returns a dictionary containing those results.

        Steps 1 and 3 must be implemented by the subclass while the others are implemented
        by EvaluationBase.
        
        Returns:
            A dictionary containing the scores for each source for the best candidate.

        &quot;&quot;&quot;
        references, estimates = self.preprocess()
        combos, orderings = self.get_candidates()
        
        best_permutation_key = self.best_permutation_key
        scores = []
        metrics = []

        for k, combo in enumerate(combos):
            _estimates = estimates[..., combo]
            for o, order in enumerate(orderings):
                _scores = self.evaluate_helper(
                    references[..., order], _estimates, **self.eval_args)

                if not best_permutation_key or best_permutation_key not in _scores[0]:
                    best_permutation_key = sorted(_scores[0].keys())[0]

                _metrics = []
                for _score in _scores:
                    _metrics.extend(_score[best_permutation_key])

                metrics.append(np.mean(_metrics))
                scores.append((combo, order, _scores))
            
        best_idx = np.argmax(metrics)
        combo, order, score = scores[best_idx]
        results = {
            &#39;combination&#39;: combo,
            &#39;permutation&#39;: order
        }

        for i, o in enumerate(order):
            results[self.source_labels[o]] = score[i]
        self._scores = results

        return results</div>

    @property
    def scores(self):
        &quot;&quot;&quot;
        A dictionary that stores all scores from the evaluation method. Gets populated when
        :func:`evaluate` gets run.

        &quot;&quot;&quot;
        return self._scores</div>


class AudioSignalListMismatchError(Exception):
    &quot;&quot;&quot;
    Error class for when true_sources_list and estimated_sources_list are different
    lengths.
    &quot;&quot;&quot;
    pass
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Ethan Manilow, Prem Seetharaman

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>